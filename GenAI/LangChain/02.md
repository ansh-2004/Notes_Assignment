# LangChain Model Component

- LangChain have 2 different types of Model :- Language Model and Embedding Model
- In Language model , we give text as input and model interprets this text and give text as output. 
- In Embedding model , we give text as input and model interprets this text and give series of numbers as output . These numbers are called embeddings and these set of numbers are called vectors . These numbers represent the contextual meaning of the input text .

- With the help of language model , we make chat bot type application
- Embedding model are useful in semantic search . So they are useful in making RAG based application. The company pdf example we have seen before is an RAG based application. 

    ````
    Semantic search is a technology that goes beyond matching keywords to understand the context and intent of a user's query, using Natural Language Processing (NLP) and machine learning to provide more relevant and accurate results. Instead of just finding exact words, it analyzes the meaning and relationships between words to deliver content that aligns with what the user is truly looking for, making the search experience more intuitive and personalized`
    ````


## Language Models
- In language models , we have two types of models:- LLMs and Chat Models 

- LLMs are general purpose models which means we can use them in any kind of NLP application . You can do text generation , text summarization , code generation , question /answering. We give LLMs string in plain text and it turns around and gives you a string in plain text . But these LLMs are now replaced by chat models.

- Chat Models are these language models used for conversation tasks. means if you want to create a chat bot , then chat models are great. In this we can put multiple messages simultaneously as input and chat model can understand this conversation and reply with a series of messages 

## Setup 

- For the first project , we need these dependencies
- langchain , @langchain/openai and openai , @langchain/anthropic, @langchain/google-genai

## 01_project/LLM

- For the LLM , we can use open ai api key but that is paid so we will use Gemini api key 

```javascript
import { OpenAI } from "@langchain/openai";

const llm = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  modelName: "text-davinci-003", // LLM model
});

const response = await llm.invoke("Write a short story about a robot learning emotions.");
console.log("LLM Output:", response);

```
- This code is for open ai 

```javascript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage } from "langchain/schema";
import * as dotenv from "dotenv";

dotenv.config();

const model = new ChatGoogleGenerativeAI({
  apiKey: process.env.GOOGLE_API_KEY,
  modelName: "gemini-1.5-flash",
});

const response = await model.invoke([
  new HumanMessage("Write a short story about a robot learning emotions.")
]);

console.log("Gemini Output:", response.content);

```

- Unlike openai ,  Gemini does not currently have a separate LLM class like OpenAI. Instead, it is accessed via the chat model interface using ChatGoogleGenerativeAI.


---

- Moving onto Chat Model as LLM model are not used nowadays
### 01_project/chatModels

#### 1. chatModel_openai.js

```javascript
import { ChatOpenAI } from "@langchain/openai";  // now you don't take just openai from @langchain/openai , you take chatopenai , it is only basic difference from LLM model 

// Also another difference is , this chatopenai is derived from basechatmodel class  , not only this all gemini , claude all are derived from this basechatmodel , But in LLMs they are derevied from basellm class    in langchain 
import { HumanMessage } from "@langchain/core/messages";
import dotenv from "dotenv";

dotenv.config();

const model = new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  modelName: "gpt-4",
});

const response = await model.invoke([
  new HumanMessage("Write a 5 line poem on cricket")
]);

console.log("OpenAI Response:", response.content);

```

#### Explanation 

## ðŸ§  What Is `ChatOpenAI` in LangChain.js?

In LangChain.js, `ChatOpenAI` is a class provided by the `@langchain/openai` package. It wraps OpenAI's **chat models** like:

- `gpt-3.5-turbo`
- `gpt-4`
- `gpt-4o`

These models are designed for **conversational tasks**, meaning they take a sequence of messages and return a message. LangChain.js makes it easy to use these models in chains, agents, and other workflows.

---

## ðŸ”§ Parameters Youâ€™ll Use

### 1. `modelName: 'gpt-4'`
- Specifies which OpenAI model to use.
- You can also use `"gpt-3.5-turbo"` or `"gpt-4o"`.

### 2. `temperature: 1.5`
- Controls **creativity** of the output.
- Range: `0` (deterministic) to `2` (very creative).
- `1.5` is high â€” good for poetry, storytelling, etc.

### 3. `maxTokens: 10`
- Limits the **length of the model's response**.
- Only 10 tokens will be generated.
- Useful for short outputs, but may cut off longer responses.

---

### ðŸ”¸ `ChatOpenAI({...})`
- Initializes the GPT-4 chat model with your chosen parameters.
- `apiKey`: Your OpenAI API key from `.env`.
- `temperature`: Controls creativity.
- `maxTokens`: Limits response length.

### ðŸ”¸ `HumanMessage(...)`
- Wraps your prompt as a message from the user.

### ðŸ”¸ `model.invoke([...])`
- Sends the message to the model and waits for a response.

### ðŸ”¸ `response.content`
- Extracts the actual text response from the model.

---

```
1. BaseLLM

Used for LLMs that take plain text as input and return plain text.
These are non-conversational models.
Examples:

OpenAI (for text-davinci-003)
LlamaCpp
GPT4All
HuggingFaceLLM



2. BaseChatModel

Used for chat models that take a sequence of messages and return a message.
These are conversational models.
Examples:

ChatOpenAI (for gpt-3.5-turbo, gpt-4)
ChatGoogleGenerativeAI (for Gemini)
ChatAnthropic (for Claude)
ChatCohere, ChatMistral, etc.
```