# LangChain Model Component

- LangChain have 2 different types of Model :- Language Model and Embedding Model
- In Language model , we give text as input and model interprets this text and give text as output. 
- In Embedding model , we give text as input and model interprets this text and give series of numbers as output . These numbers are called embeddings and these set of numbers are called vectors . These numbers represent the contextual meaning of the input text .

- With the help of language model , we make chat bot type application
- Embedding model are useful in semantic search . So they are useful in making RAG based application. The company pdf example we have seen before is an RAG based application. 

    ````
    Semantic search is a technology that goes beyond matching keywords to understand the context and intent of a user's query, using Natural Language Processing (NLP) and machine learning to provide more relevant and accurate results. Instead of just finding exact words, it analyzes the meaning and relationships between words to deliver content that aligns with what the user is truly looking for, making the search experience more intuitive and personalized`
    ````


## Language Models
- In language models , we have two types of models:- LLMs and Chat Models 

- LLMs are general purpose models which means we can use them in any kind of NLP application . You can do text generation , text summarization , code generation , question /answering. We give LLMs string in plain text and it turns around and gives you a string in plain text . But these LLMs are now replaced by chat models.

- Chat Models are these language models used for conversation tasks. means if you want to create a chat bot , then chat models are great. In this we can put multiple messages simultaneously as input and chat model can understand this conversation and reply with a series of messages 

## Setup 

- For the first project , we need these dependencies
- langchain , @langchain/openai and openai , @langchain/anthropic, @langchain/google-genai

## 01_project/LLM

- For the LLM , we can use open ai api key but that is paid so we will use Gemini api key 

```javascript
import { OpenAI } from "@langchain/openai";

const llm = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  modelName: "text-davinci-003", // LLM model
});

const response = await llm.invoke("Write a short story about a robot learning emotions.");
console.log("LLM Output:", response);

```
- This code is for open ai 

```javascript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { HumanMessage } from "langchain/schema";
import * as dotenv from "dotenv";

dotenv.config();

const model = new ChatGoogleGenerativeAI({
  apiKey: process.env.GOOGLE_API_KEY,
  modelName: "gemini-1.5-flash",
});

const response = await model.invoke([
  new HumanMessage("Write a short story about a robot learning emotions.")
]);

console.log("Gemini Output:", response.content);

```

- Unlike openai ,  Gemini does not currently have a separate LLM class like OpenAI. Instead, it is accessed via the chat model interface using ChatGoogleGenerativeAI.


---

- Moving onto Chat Model as LLM model are not used nowadays
### 01_project/chatModels

#### 1. chatModel_openai.js

```javascript
import { ChatOpenAI } from "@langchain/openai";  // now you don't take just openai from @langchain/openai , you take chatopenai , it is only basic difference from LLM model 

// Also another difference is , this chatopenai is derived from basechatmodel class  , not only this all gemini , claude all are derived from this basechatmodel , But in LLMs they are derevied from basellm class    in langchain 
import { HumanMessage } from "@langchain/core/messages";
import dotenv from "dotenv";

dotenv.config();

const model = new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  modelName: "gpt-4",
});

const response = await model.invoke([
  new HumanMessage("Write a 5 line poem on cricket")
]);

console.log("OpenAI Response:", response.content);

```

#### Explanation 

## üß† What Is `ChatOpenAI` in LangChain.js?

In LangChain.js, `ChatOpenAI` is a class provided by the `@langchain/openai` package. It wraps OpenAI's **chat models** like:

- `gpt-3.5-turbo`
- `gpt-4`
- `gpt-4o`

These models are designed for **conversational tasks**, meaning they take a sequence of messages and return a message. LangChain.js makes it easy to use these models in chains, agents, and other workflows.

---

## üîß Parameters You‚Äôll Use

### 1. `modelName: 'gpt-4'`
- Specifies which OpenAI model to use.
- You can also use `"gpt-3.5-turbo"` or `"gpt-4o"`.

### 2. `temperature: 1.5`
- Controls **creativity** of the output.
- Range: `0` (deterministic) to `2` (very creative).
- `1.5` is high ‚Äî good for poetry, storytelling, etc.

### 3. `maxTokens: 10`
- Limits the **length of the model's response**.
- Only 10 tokens will be generated.
- Useful for short outputs, but may cut off longer responses.

---

### üî∏ `ChatOpenAI({...})`
- Initializes the GPT-4 chat model with your chosen parameters.
- `apiKey`: Your OpenAI API key from `.env`.
- `temperature`: Controls creativity.
- `maxTokens`: Limits response length.

### üî∏ `HumanMessage(...)`
- Wraps your prompt as a message from the user.

### üî∏ `model.invoke([...])`
- Sends the message to the model and waits for a response.

### üî∏ `response.content`
- Extracts the actual text response from the model.

---

```
1. BaseLLM

Used for LLMs that take plain text as input and return plain text.
These are non-conversational models.
Examples:

OpenAI (for text-davinci-003)
LlamaCpp
GPT4All
HuggingFaceLLM



2. BaseChatModel

Used for chat models that take a sequence of messages and return a message.
These are conversational models.
Examples:

ChatOpenAI (for gpt-3.5-turbo, gpt-4)
ChatGoogleGenerativeAI (for Gemini)
ChatAnthropic (for Claude)
ChatCohere, ChatMistral, etc.
```


### correct parameter for maxtokens 

---

### ‚úÖ Token Limit Parameters by Model

| Model Type        | LangChain.js Class              | Correct Parameter Name     | Description |
|-------------------|----------------------------------|-----------------------------|-------------|
| **OpenAI**        | `ChatOpenAI`                    | `maxTokens` or `maxCompletionTokens` | Controls the maximum number of tokens in the output. Both are supported[1](https://v03.api.js.langchain.com/interfaces/_langchain_openai.ChatOpenAIFields.html). |
| **Anthropic (Claude)** | `ChatAnthropic`                  | `maxTokens`                 | Controls the maximum number of output tokens[2](https://js.langchain.com/docs/integrations/chat/anthropic/). |
| **Google Gemini** | `ChatGoogleGenerativeAI`        | `maxOutputTokens`           | Controls the maximum number of output tokens[3](https://v03.api.js.langchain.com/classes/_langchain_google_genai.ChatGoogleGenerativeAI.html). |


---



---

# Hugging face 

---

## üß† Python Code Recap

```python
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from dotenv import load_dotenv

load_dotenv()

llm = HuggingFaceEndpoint(
    repo_id="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    task="text-generation"
)

model = ChatHuggingFace(llm=llm)

result = model.invoke("What is the capital of India")

print(result.content)
```

---

## ‚úÖ Node.js Equivalent (LangChain.js)

### 1. **Install Required Packages**

```bash
npm install @langchain/community @langchain/core @huggingface/inference dotenv
```


### 3. **Code Implementation**

```js
import { config } from 'dotenv';
import { HuggingFaceInference } from '@langchain/community/llms/hf';

config(); // Load .env variables

const model = new HuggingFaceInference({
  model: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0', // ‚úÖ same as repo_id in Python
  apiKey: process.env.HUGGINGFACEHUB_API_KEY,  // ‚úÖ pulled from .env
  task: 'text-generation',                     // ‚úÖ same as Python
});

async function main() {
  const res = await model.invoke('What is the capital of India');
  console.log(res); // This prints the full response object
}

main();
```

---

## üß† Explanation of Each Part (with Docs)

| Component | Purpose | Reference |
|----------|---------|-----------|
| `HuggingFaceInference` | LangChain.js wrapper for Hugging Face models | [LangChain.js Docs](https://js.langchain.com/docs/integrations/llms/huggingface_inference/)ÓàÄciteÓàÇturn9search33ÓàÅ |
| `model` | Hugging Face model ID (same as `repo_id` in Python) | Official Hugging Face model repo |
| `task` | Defines the type of model (e.g., `text-generation`, `text2text-generation`) | Hugging Face API |
| `apiKey` | Authenticates your requests to Hugging Face | Loaded via `.env` |
| `invoke()` | Sends a prompt and returns the model's response | LangChain.js standard method |

---

## üîç Notes

- The `ChatHuggingFace` abstraction in Python is not directly mirrored in LangChain.js ‚Äî instead, you use `HuggingFaceInference` directly.
- You can wrap this in a LangChain `Runnable` or chain if needed.
- Streaming is not supported for all Hugging Face models via the Inference API.

---
Why ChatHuggingFace Is Missing in JS?
In Python, ChatHuggingFace is a higher-level abstraction that wraps around HuggingFaceEndpoint to provide chat-style formatting. In LangChain.js:

There is no separate ChatHuggingFace class.
You use HuggingFaceInference directly for both chat and text-generation models.
You manually format prompts if needed for chat-style models.


```
error i getting

Defaulting to 'auto' which will select the first provider available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.
Auto selected provider: undefined
```

```
Great question, Ansh! You're absolutely right ‚Äî in Python, when using LangChain or Hugging Face models, you often don‚Äôt need to worry about **inference providers** explicitly. But in **JavaScript/TypeScript**, especially when using Hugging Face's hosted models via their **Inference API**, this concept becomes more visible.

Let‚Äôs break it down step-by-step:

---

## üîç What Is "Inference"?

**Inference** means:  
> Running a trained machine learning model to get predictions or outputs.

For example:
- You send a prompt to a language model ‚Üí it returns a response.
- You send an image to a vision model ‚Üí it returns a caption or classification.

This process is called **inference**, and it needs **compute resources** (like GPUs or CPUs) to run the model.

---

## üß© What Is a "Provider"?

A **provider** is the **backend infrastructure** that actually runs the model for you.

### Hugging Face supports multiple providers:
- **Hugging Face-hosted** (default)
- **AWS SageMaker**
- **Azure ML**
- **Google Cloud**
- **Custom endpoints**

Each provider has different performance, cost, and availability characteristics.

---

## üß† Why Do You Need a Provider in Node.js?

In **Node.js**, when you use Hugging Face's hosted models via their **Inference API**, you‚Äôre not running the model locally ‚Äî you're asking Hugging Face to run it **on your behalf**.

So Hugging Face needs to know:
> ‚ÄúWhich backend should I use to run this model?‚Äù

If you don‚Äôt specify one, it tries to **auto-select** from your list of providers in your Hugging Face settings.

If **no provider is configured**, it shows:
```
Auto selected provider: undefined
```

---

## üêç Why Python Doesn‚Äôt Need This (Usually)

In Python:
- You often run models **locally** (e.g., using `transformers` with `pipeline()`).
- Or you use Hugging Face‚Äôs hosted endpoints **without needing to manage providers** ‚Äî they‚Äôre abstracted away.

LangChain Python also has tighter integrations with Hugging Face, so it handles these details internally.

---

## ‚úÖ Summary

| Concept | Node.js (Hugging Face Inference API) | Python |
|--------|----------------------------------------|--------|
| Inference | Done remotely via API | Often done locally or via abstracted API |
| Provider | Must be configured or auto-selected | Usually hidden or not needed |
| Error | `Auto selected provider: undefined` if none set | Rarely seen |

---

Would you like help setting up a provider in Hugging Face or switching to a different model that doesn‚Äôt require this? I can walk you through either option.
```
