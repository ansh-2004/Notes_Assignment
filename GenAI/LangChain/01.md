- Embeddings, or vectors, are numerical representations of data—like words, images, or sounds—in a high-dimensional space. They are mathematical arrays of numbers generated by an embedding model, with similar data points mapped closer together to capture their semantic meaning and relationships. This numerical format allows machine learning models to process and analyze complex, unstructured data, enabling tasks such as semantic search, text translation, and recommendation systems


- The brain we are talking about in page 8 , is nothing but LLMs . And It is difficult to use LLMs in our own server because they require high compution, so what LLMs companies like openai , anthorpy do , they said we run LLMs in our own servers , they provide us API , Using API we can talk to these LLMs . So brain is nothing but LLM API .

### Chains in LangChain

- They are used to build pipelines in LangChain 
- We build an LLM application and give it a shape of a pipeline   
- And the pipiline that will be created , you can create it with the help of chains. 
- For example, you want to create such an LLM application where the user will give you a big Englist text as an input of around 1000 words. And in the output you have to give its hindi summary in less than 100 words. 
- So the flow of this LLM application will be that first you will get input , you will send this input to an LLM and LLM will translate this input in Hindi.  
- Now this translated text will send to a second LLM and this LLM will generate its Hindi summary in less than 100 Words . 
- Now we can easily represent this flow in the form of pipeline. 
- Now if we don't use chains , then we need to design this entire pipeline manually, which means you take input from the user, then you will call first LLM, provide it with the input , and tell him to translate it into Hindi ,then go to second LLM and tell him to generate the summary of this hindi translation and then you get final output . 
- Means you have to manually take the output from every stage from every place and manually put in the input of the next stage.
- Chains solve this problem by automatically make the output of one stage , the input of other stage and no need to write any code manually for this 
- With chains , you have to just pass english text as input and call the chain . Behind the scenes this entire task will be executed automatically and here you will get your result . 
- Hence , chain is the concept with the help of which you can create a pipeline in a langchain.


### Indexes
- There comes 4 things inside indexes:- Doc Loader, text splitter, vector store, Retrievers. These 4 things combine to make indexes
- Now take example of chatgpt, it is trained on whole internet data, but if we ask it about leave policy of xyz company or notice policy of xyz company , then chatgpt will not be able to answer this question
- It is because chatgpt has not seen this data during its training as it is private data of a company 
- So to provide llm our personal data , we can connect LLM to an external knowledge source like we can provide complete rule book to the LLM . 
- Hence , indexes are used to build such applications 

- in the flow chart example , **doc loader** is used to load the pdf or file from s3, google drive or anywhere where it is stored. 
- Now when the data is loaded , to allow semantic search on this , we divide the entire document into small chunks , on the basis of pages or paragraph or chapters etc. This chunking operation is done by **text splitter**
- Now we convert our document into vectors so that semantic search is possible . For each of the pages we create embedding using any embedding model .
- Now we have 1000 embedding vectors for these 1000 pages . Now to store these vectors , we need database which is called **vector database or vector store**.
- Now when a query from user comes like what is the leave policy of the company , then the fourth component of indexes comes into play which is **Retreivers**.  Retreivers takes that user query , generates its embedding with the help of same embedding model and then vector receives from this is used for semantic search on the vector database . Once its gets relavent result from this semantic search , retriver give this result and the user query to LLM  and LLM now replies to this .    


### Memory 

- LLM API calls are stateless .  It means every request we hit to LLM is independent means it does not have any memory about previous request . 
- In the given example , we see when we asked who is narendar modi we get response but next when we ask how old is he , we don't get response , It is because our llm does not know who 'he' is there.
- So this problem is solved by this memory component of LangChain . 

- ConversationBufferMemory: Stores a transcript of recent messages. Great for 
short chats but can grow large quickly.
    - In this we store all the conservations that have happened till now and when you are making next api call then you send this entire chat history to the model so then he understand what is being talked about  . But only problem is that if chat goes bigger then more time in processing which costs you more 

- ConversationBufferWindowMemory: Only keeps the last N interactions to avoid 
excessive token usage.
    - In this we store and send only last N interactions to next Api call 

- Summarizer-Based Memory: Periodically summarizes older chat segments to keep 
a condensed memory footprint.

- Custom Memory: For advanced use cases, you can store specialized state (e.g., 
the user’s preferences or key facts about them) in a custom memory class.

### Agents

- This component of langchain helps us to create AI agents .  
- Now we know LLMs have 2 main features :- NLU + Text generation . Which means LLMs can understands the text easily and in return of understanding is able to generate the correct text as well 
- So when LLMs come , its main use case is making chatbots like chatgpt etc. 
- Now think if chatbot can understand and reply to our text then it can also do some work in return. for eg, I am talking to chat bot on travel website , and i ask which is best travel destination in india during summer time , then this chatbot can reply to this easily . But if this is AI agent instead of only chatgpt then i can ask which is the cheapest flight between delhi and amritsar on 24 april then this AI agent not only reply to this but also can book our flight easily. 

- So what extra things AI agent have than chatbot?
- it is reasoning capability and access to tools like hit that api etc. 
- Due to these , AI agent can work for us 

- Suppose we have ai agent , we give him two tool -> calculator and access to weather api . now a user comes and talk to our ai agent that can you mulitply the  today's temperature of delhi with 3. Now as AI agent has reasoning capability , it know what to do . There are many techniques of reasoning. One of the most popular is **Chain of thought prompting**. In this AI agent break down your query step by step and then try to reason through it . 
- It breaks the query into first get delhi temp for today and then multiply by 3 
- Now it check whether it has any tool to get delhi temp, so as he have he get that . Now he have to multiply it by 3 so now he check whether he have any tool for this purpose , as he have calculator so he will quickly call this calculator by giving input as temp,3, * and now he will give the result. 